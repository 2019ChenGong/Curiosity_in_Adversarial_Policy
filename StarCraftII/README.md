# StarCraft II Games

All the code files you need to replicate our experiments are released in the folder ```src```.

For configurations, please pull and install the repo from: https://github.com/Tencent/PySC2TencentExtension. Then, run 
```
pip install -r requirements.txt
```
It is noticed that the version of StarCraft II is SC2 4.6.2 (B69232). For the installation, pls run
```
cd ~
wget https://blzdistsc2-a.akamaihd.net/Linux/SC2.4.6.2.69232.zip
unzip SC2.4.6.2.69232.zip # Password is iagreetotheeula
```
Besides, the readme file of how to install the SC (https://github.com/deepmind/pysc2/blob/master/README.md) explain the detail to download map sources, 
https://github.com/Blizzard/s2client-proto#downloads.


## Training Curiosity-Driven and Victim-Aware Adversarial Policies:

- Our attack: 
To start the learner:
```
python -m bin.advtrain_ppo --job_name learner --init_model_path '../normal-agent/checkpoint-100000' --save_dir <path-to-a-folder>
``` 

Then start the actor:
```
for i in $(seq 0 20); 
    do python -m bin.advtrain_ppo --job_name=actor \
       --init_model_path '../normal-agent/checkpoint-100000' --learner_ip localhost & 
done;
```
It is noticed that `20` refers to the number of actors.

- Baseline attack: 
To start the learner:
```
python -m bin.advtrain_baseline --job_name learner --init_model_path '../normal-agent/checkpoint-100000' --save_dir <path-to-a-folder>
```
To start the actor:
```
for i in $(seq 0 20); 
    do python -m bin.advtrain_baseline --job_name=actor \
       --init_model_path '../normal-agent/checkpoint-100000' --learner_ip localhost & 
done;
```
It is noticed that `20` refers to the number of actors.

## Retraining of Victim Agents:

⚠️ Change the 52, 53, 84 lines of file `bin/adv_mixretrain_rnd.py` to assign the path of the adversarial agent, norm agent, and victim agent, respectively. 

Then, to start the learner: 
```
python -m bin.adv_mixretrain_rnd --job_name learner --save_dir <path-to-a-folder> &
``` 

To start the actor:
```
for i in $(seq 0 20); 
    do python -m bin.adv_mixretrain_rnd --job_name=actor --learner_ip localhost & 
done;
```
It is noticed that `20` refers to the number of actors.

## Evaluation:
To play against an adversarial agent with a victim to calculate the winning and non-loss rates of adversarial agent:
```
python -m bin.evaluate_vs_rl --model_path=<path-of-the-adversarial-agent> --victim_path=<path-of-the-victim-agent> --mask_victim=False
```

To play against an adversarial with a masked victim:
```
python -m bin.evaluate_vs_rl --model_path=<path-of-the-adversarial-model> --victim_path=<path-of-the-victim-model> --mask_victim=True
``` 

## Visualizing the winning and non-loss rate of the adversarial agents/retrained victim agents:

Move the result file (Log.txt) generated by training adversarial policy to a targeted folder.
```
mv Log.txt ../our_result/
```

Change the line 6 in the file `our_plot.py` to assign the folder of results, then: 
```
python our_plot.py
```
e.g., pls alter the line 6 to "./total_results" and then run "python our_plot.py". You can get the visualization results.

## Visualizing the t-SNE:
To collect the victim activations when playing against different opponents:
```
python -m bin.generate_activations --model_path=<path-of-the-opponent-agent> --victim_path=<path-of-the-victim-agent> --out_path=<output-folder>
```

To generate the results of t-SNE visualization:
```
python plot_tsne.py --dir <path-to-victim-activations> --output_dir <output-folder>
```
